# Hogwild! with Distributed Cache

## Introduction
`Stochastic Gradient Descent (SGD)` is a commonly used algorithm, especially in the context of large-scale machine learning problems. The challenge is to effectively parallelize SGD on multiple machines while overcoming problems like latency and synchronization issues. This led to the development of `Hogwild!`, which is an asynchronous lock-free version of SGD. While `Hogwild!` outperforms methods that employ locking, it in turn suffers from scalability issues on `NUMA` (Non-Uniform Memory Access) systems with a large number of cores.  This is due to issues like cache coherence under the MESIF protocol, and false sharing of data between multiple cores. There have been previous attempts in the literature to overcome Hogwild!’s bottleneck, resulting from its single shared global model that is accessed by all cores. 

`HogWild++` proposes replacing the global model with a set of local models shared by clusters of nodes. This decentralization aims to minimize expensive calls to remote memory by confining thread synchronization among neighbors via a token-sharing mechanism. Much of the paper’s efforts focus on implementing a clustering strategy to reduce sharing between all threads. The convergence issue with local models is not handled by a separate synchronization thread, unlike in `DimmWitted`. This is because `NUMA` systems discourage frequent access to remote memory. Instead, clusters under this system are only able to communicate with their socket neighbors, which is a low-cost operation. Each cluster has effectively 2 local models: one model is the snapshot after the last sync, and the other is being constantly updated. They define the difference between these 2 models, and mention that the updated model is the one that is always used for gradient computation. Whenever the cluster gets hold of the token, it passes this difference along to its neighbor and updates its own snapshot. There is also a decay factor introduced to prevent the local weights from blowing up to infinity. Intuitively, older weights are supposed to have ‘lesser information’, and so should contribute less to the update.

`Dogwild!` is a distributed architecture of `HogWild!`, based on a Master-Slave model. The idea of `Hogwild!`  is extended in two directions:  increased decoupling between computation and data streaming, and tolerating occasional gradient loss in order to use unreliable and multicast network transports. The model weights are replicated on all nodes in the cluster. The master loops over the weight buffer, sending a set of weights at a time to all slaves. Each slave compares its own weights to a copy, unchanged since the last message from the master. The difference is thus the gradient since the last synchronization. The slave updates its weights using the master position plus the gradient, and sends the gradient to the master.  Then the master multicasts to all slaves to synchronize the weight vector. Issues with `Dogwild!’s` approach pertain to the high communication cost to synchronize the weight vectors locally in all slaves. Performance comparisons with other distributed asynchronous systems are missing, as is detail about how the weights are transferred concurrently to master.

`Parameter server` architecture has two classes of nodes. The server nodes maintain a partition of the globally shared parameters (machine local parameters are not synchronized by default). They communicate with each other to replicate and/or to migrate parameters for reliability and scaling. The client nodes perform the bulk of the computation; the server nodes mainly perform bookkeeping and global aggregation steps. Each client typically stores locally a portion of the training data, computing local statistics such as gradients.  Clients communicate only with the server nodes, updating and retrieving the shared parameters. Clients may be added or removed; doing so requires transmitting the appropriate portion of the training dataset to the new machine(s) and querying the respective set of parameters. However, the literature does not highlight communication costs involved to pull and push the parameters from the client to the server.

## Problem Statement
The broad aim of this project is to run Hogwild! on multiple machines with the aid of a `distributed cache`.  We aim to extend `Hogwild!’s` original environment (multiple cores on a single system) to multiple systems via the use of distributed cache.
Note that there is a resultant tradeoff we are making implicitly: that between the speedup achieved using the multiple threads, versus the benefits of solving cache coherence problems in the single-threaded environment. An empirical study of this tradeoff might go some way to establishing best practices in such scenarios. Prior literature is unclear on the choice, or indeed what form the tradeoff might take, which this project will aim to shed light on. Following the distributed single-threaded implementation of `Hogwild!`, we also aim to analyze the performance of the algorithm in this new setup in terms of theoretical guarantees like convergence rates. We stress that our approach is highly empirical to begin with, since we are very dependent on data gathered during the distributed single-threaded implementation for our subsequent analyses.

## Approach, Methodology, Directions
The preliminary setup involves utilizing a Redis cluster to run `Hogwild!` on a chosen benchmark dataset. This is done to compare its observed performance with that of `Hogwild!` run on multiple cores on a single machine. Our motivation for using a Redis cluster lies in the following gains: it is fault-tolerant, horizontally scalable (which allows addition to the cluster with ease), and it renders an increase in the effective cache size. The proposed setup involves storing a single global model in the distributed cache, with data (both training and test) stored in every single node. To account for uniformity in shuffling and ensure the holdout nature of the test data set, data partitioning is achieved prior to saving at the node locations. Empirical data is then collected from the setup once it is run. This includes measures of runtime and test set accuracy (held fixed to a threshold and checked against variable runtime required to achieve it). There is an argument to be made for fixing the benchmark dataset to run `Hogwild!` on, due to data dimensionality and sparsity. If, for instance, the dataset is high-dimensional and extremely sparse, but the sparsity results in local concentrations of non-zero values, then we are restricted to frequent weight updates in a selected small subset of the model. The overall effect is then akin to training a small model, which might serve as a bottleneck.  Recall that `Hogwild!`  itself scales best when we update large models with sparse data, in conjunction with a uniform distribution of the non-zero values.
